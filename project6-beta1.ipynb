{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project 6: Analyzing Stock Sentiment from Twits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "\n",
    "<Story>  \n",
    "    There is \n",
    "Each problem consists of a function to implement and instructions on how to implement the function. The parts of the function that need to be implemented are marked with a # TODO comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our project_tests package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like Torch, NLTK. These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "The other packages that we're importing are project_helper and project_tests. These are custom packages built to help you solve the problems. The project_helper module contains utility functions and graph functions. The project_tests contains the unit tests for all the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import project_tests\n",
    "import project_helper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (~Subject to change) \n",
    "\n",
    "**Parni's suggestions** \n",
    "\n",
    "In this project, your aim is to build a model that can predict the sentiment of the text in the twits. \n",
    "\n",
    "In order to this should follow the four steps: \n",
    "1- Import the data \n",
    "2- Preprocess your data \n",
    "3- Building your NN model \n",
    "4- Make a prediction for your model \n",
    "\n",
    "\n",
    "**End** \n",
    "\n",
    "\n",
    "You've been considering using sentiment around specific stocks in your models. You've been subscribed to StockTwits for a while to stay up to date on trading news. You can collect these twits (similar to tweets) and now you want to build a model that can predict the sentiment of the text in the twits. Many of the existing models perform feature extraction manually, basically assigning sentiment scores to individual words by hand. Instead you'd like to train a neural network to learn the features itself then have it predict sentiment. This means you'll need labeled data.\n",
    "\n",
    "You collected a bunch of twits, then hand labeled the sentiment of each with the help of some interns. You wanted to capture the degree of sentiment so you decided to use a five-point scale: very negative, negative, neutral, positive, very positive. Each tweet is labeled -2 to 2 in steps of 1, from very negative to very positive. \n",
    "\n",
    "Here then, you'll build a sentiment analysis model that will learn to assign sentiment to tweets on its own, using this labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"image2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the twits. This is a JSON object with structure like so:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Tweet body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy tweet body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import Twits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twits.json', 'r') as f: #say if this is label data or not \n",
    "    twits = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:06:01Z'},\n",
       " {'message_body': '$GOOG http://stks.co/1jQs Many market leaders appear extended. Are these moves sustainable? I think not.',\n",
       "  'sentiment': -1,\n",
       "  'timestamp': '2012-01-01T00:18:17Z'},\n",
       " {'message_body': '\"Deconstructing A Trade: $AAPL 12/29/2011\"-New Blog Post.  Yeah it\\'s New Year\\'s Eve but I\\'m married with kids http://t.co/6VV31tBY $STUDY',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:26:18Z'},\n",
       " {'message_body': 'My prediction for 2012 is that the $spx and $djia ($spy and $dia) will make all time highs. And $aapl right along with them.',\n",
       "  'sentiment': 2,\n",
       "  'timestamp': '2012-01-01T00:30:36Z'},\n",
       " {'message_body': 'RT @bclund &quot;Deconstructing A Trade: $AAPL 12/29/2011&quot;New Blog Post. Yeah it&#39;s NY&#39;s Eve but I&#39;m married with kids http://stks.co/1jQy $STUDY',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:34:52Z'},\n",
       " {'message_body': 'RT @bclund: \"Deconstructing A Trade: $AAPL 12/29/2011\"-New Blog Post. http://t.co/aJGevn88 $STUDY // Sick post. pure gold. 8 beers already?',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:49:22Z'},\n",
       " {'message_body': 'Ten Great New Yearâ€™s Resolutions for Traders http://stks.co/1jRA  $study These resolutions apply whether you are trading $SPY or $AAPL',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:59:12Z'},\n",
       " {'message_body': '@howardlindzon Funny check all your contacts too... use this list \"other contacts\" to see who google+ pushed in. $GOOG',\n",
       "  'sentiment': 1,\n",
       "  'timestamp': '2012-01-01T01:35:57Z'},\n",
       " {'message_body': 'Bank CEOs Earn Big Bucks Even as Stocks Get Slammed\\n http://t.co/R9TxBohX $JPM $GS $BAC',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T01:56:16Z'},\n",
       " {'message_body': 'Prices are LOCKED on our 10 buy-and-holds for 2012 - $AA $ARCO $CAT $COF $FDX $HSY $MAKO $MSFT $STD $TKC http://stks.co/1jRN #beststocks',\n",
       "  'sentiment': 2,\n",
       "  'timestamp': '2012-01-01T02:22:38Z'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twits['data'][0:10]# loading 10 tweets from the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out one tweet and look at the fields. Fields in our individual tweets:\n",
    "\n",
    "* `'message_body'`: The actual text in the tweet\n",
    "* `'sentiment'`: Score on the sentiment of the tweet, ranges from -2 to 2 in steps of 1, with 0 being neutral\n",
    "\n",
    "Remember that we want our network to look at some text and predict the sentiment. Our training input will be the message bodies, and we can use the sentiment score as training label for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twits['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 million tweets over all. For development purposes, let's only use the first 2 million tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = twits['data'][:2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data we'll train & test on\n",
    "messages = [twit['message_body'] for twit in data]\n",
    "# Adding 2 here to scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Preprocessing\n",
    "\n",
    "\n",
    "With our data in hand we need to preprocess our text. These tweets are collected by filtering on ticker symbols where these are denoted with a leader $ symbol in the tweet itself. For example,\n",
    "\n",
    "`{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}`\n",
    "\n",
    "\n",
    "The ticker symbols don't provide information on the sentiment, and they are in every tweet, so we should remove them. This tweet also has the `@google` username, again not providing sentiment information, so we should also remove it. And, we see a URL `http://t.co/sptHOAh8`, let's remove these too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to remove specific words or phrases is with regex, the `re` module. You can sub out specific patterns with a space:\n",
    "\n",
    "```python\n",
    "re.sub(pattern, ' ', text)\n",
    "```\n",
    "This will substitute a space with anywhere the pattern matches in the text. Later when we tokenize the text, we'll split appropriately on those spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pbarekatain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(message):\n",
    "    ''' This function takes a string as input, then performs these operations: \n",
    "        * lowercase\n",
    "        * remove URLs\n",
    "        * removes punctuation\n",
    "        * tokenize\n",
    "        * removes single character tokens\n",
    "    ''' \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    # Lowercase \n",
    "    text = message.lower()\n",
    "    \n",
    "    # Match and remove URLs\n",
    "    text = re.sub('http\\S+', ' ', text)\n",
    "    \n",
    "    # Match and remove ticker symbols that start with $\n",
    "    text = re.sub('\\$\\S{2,5}', ' ', text)\n",
    "    \n",
    "    # Match and remove twitter usernames that start with @\n",
    "    text = re.sub('@\\S+', ' ', text)\n",
    "\n",
    "    # Replace punctuation and numbers (anything not a letter) with spaces\n",
    "    text = re.sub('[^A-Za-z]', ' ', text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Lemmatize and remove any tokens with only one character\n",
    "    tokens = [wnl.lemmatize(token, pos='n') for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can preprocess each of the twits in our dataset. This will take a while since we have millions of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [preprocess(message) for message in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the vocabulary\n",
    "\n",
    "Now with all of our messages tokenized, we want to create a vocabulary and count up how often each word appears in our entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "bow = Counter()\n",
    "for tokens in tokenized:\n",
    "    bow.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove common and rare words\n",
    "\n",
    "With our vocabulary, now we'll remove some of the most common words such as 'the', 'and', 'it', etc. These words don't contribute to identfying sentiment and are really common, resulting in a lot of noise in our input. If we can filter these out, then our network should have an easier time learning. It's up to you to decide how many to remove.\n",
    "\n",
    "We also want to remove really rare words that show up in a only a few tweets. Here you'll want to divide the count of each word by the number of messages. Then remove words that only appear in some small fraction of the messages. Again, it's up to you how much you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'of', 'on', 'is', 'to', 'in', 'quot', 'the', 'and', 'it'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22810"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filter high and low frequency words\n",
    "# Frequency of words appearing in messages\n",
    "freqs = {key: val/len(tokenized) for key, val in bow.items()} \n",
    "low_cutoff = 0.000005 # frequency cutoff, this is probably better as a percent of the low frequency words\n",
    "\n",
    "# Filter high frequency words\n",
    "# Here I'm setting the number of high frequency words instead of a frequency threshold.\n",
    "# The distribution of word counts is peaked at the most frequent words with a really long tail\n",
    "# So it's usually better just cut off the first K words\n",
    "high_cutoff = 10 # K high frequency words \n",
    "k_most_common = {each[0] for each in bow.most_common(high_cutoff)}\n",
    "print(k_most_common)\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in k_most_common)]\n",
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: word_id for word_id, word in enumerate(filtered_words, 1)}\n",
    "id2vocab = {val: key for key, val in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all the data and remove words that aren't in our vocab\n",
    "filtered = [[token for token in message if token in vocab] for message in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing the classes\n",
    "\n",
    "Let's do a few last pre-processing steps. If we look at how our tweets are labeled, we'll find that 50% of them are neutral. This means that our network will be 50% accurate just by guessing 0 every single time. To help our network learn appropriately, we'll want to balance our classes. That is, make sure each of our different sentiment scores show up roughly as frequently in the data. What we can do here is go through each of our examples and randomly drop tweets with neutral sentiment. What should be the probability we drop these tweets if we want to get around 20% neutral tweets starting at 50% neutral?\n",
    "\n",
    "We should also take this opportunity to remove messages with length 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1976361248285764"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the NN Network\n",
    "\n",
    "#### Add description with images of RNN (~Subject to change + diagram)\n",
    "#### 1. TextClassifier -- already make\n",
    "#### 2. Forward Pass -- ask student to build this - know more about LSTM excerise \n",
    "#### 3. Hidden Layer -- ask student to build this \n",
    "#### 4. Make our models -- ask student to build ( tell them the input that your layer get) === embed_size, lstm_size, output_size\n",
    "\n",
    "Now we have our vocabulary which means we can transform our tokens into ids, which are then passed to our network. So, let's define the network now!\n",
    "\n",
    "TODO: Have a nice diagram showing the network we'd like to build\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.embedding(input)\n",
    "        #print(embed.shape)\n",
    "        lstm_out, hidden = self.lstm(embed, hidden)\n",
    "        \n",
    "        # We just want the last sequence step output which we'll use to classify\n",
    "        # lstm_out shape is (sequence_steps, batch_size, hidden_units)\n",
    "        #print(lstm_out)\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        #print(lstm_out)\n",
    "\n",
    "        # Output fully-connected layer\n",
    "        out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        logps = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return logps, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7422, -1.4971, -1.8657, -1.7451, -1.3031],\n",
      "        [-1.6957, -1.4752, -1.8318, -1.4041, -1.7033],\n",
      "        [-1.7506, -1.7483, -1.8730, -1.4560, -1.3264],\n",
      "        [-1.8018, -1.5647, -1.9210, -1.4892, -1.3711]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as a batch, that is some number of sequences all at the same time. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
    "\n",
    "If we set our sequence length to 40, what do we do with messages that are more or less than 40 tokens? For messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "    \n",
    "    total_sequences = len(messages)\n",
    "    \n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "\n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = int(len(token_ids)*.9)\n",
    "\n",
    "train_features = token_ids[:valid_split]\n",
    "valid_features = token_ids[valid_split:]\n",
    "\n",
    "train_labels = sentiments[:valid_split]\n",
    "valid_labels = sentiments[valid_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Train loss: 1.218, Val. Loss: 1.072, Val Acc.: 59.000\n",
      "Train loss: 0.952, Val. Loss: 0.968, Val Acc.: 63.000\n",
      "Train loss: 0.867, Val. Loss: 0.912, Val Acc.: 66.000\n",
      "Train loss: 0.818, Val. Loss: 0.879, Val Acc.: 68.000\n",
      "Train loss: 0.784, Val. Loss: 0.868, Val Acc.: 68.000\n",
      "Train loss: 0.758, Val. Loss: 0.854, Val Acc.: 69.000\n",
      "Train loss: 0.749, Val. Loss: 0.834, Val Acc.: 70.000\n",
      "Train loss: 0.752, Val. Loss: 0.834, Val Acc.: 69.000\n",
      "Train loss: 0.737, Val. Loss: 0.817, Val Acc.: 70.000\n",
      "Train loss: 0.729, Val. Loss: 0.816, Val Acc.: 70.000\n",
      "Train loss: 0.712, Val. Loss: 0.803, Val Acc.: 70.000\n",
      "Train loss: 0.715, Val. Loss: 0.796, Val Acc.: 71.000\n",
      "Train loss: 0.716, Val. Loss: 0.799, Val Acc.: 70.000\n",
      "Train loss: 0.706, Val. Loss: 0.807, Val Acc.: 71.000\n",
      "Train loss: 0.705, Val. Loss: 0.802, Val Acc.: 71.000\n",
      "Train loss: 0.702, Val. Loss: 0.807, Val Acc.: 71.000\n",
      "Train loss: 0.697, Val. Loss: 0.799, Val Acc.: 71.000\n",
      "Train loss: 0.703, Val. Loss: 0.806, Val Acc.: 71.000\n",
      "Train loss: 0.696, Val. Loss: 0.788, Val Acc.: 71.000\n",
      "Train loss: 0.709, Val. Loss: 0.775, Val Acc.: 71.000\n",
      "Train loss: 0.698, Val. Loss: 0.781, Val Acc.: 71.000\n",
      "Train loss: 0.692, Val. Loss: 0.787, Val Acc.: 71.000\n",
      "Train loss: 0.684, Val. Loss: 0.786, Val Acc.: 71.000\n",
      "Train loss: 0.691, Val. Loss: 0.790, Val Acc.: 71.000\n",
      "Train loss: 0.684, Val. Loss: 0.790, Val Acc.: 71.000\n",
      "Train loss: 0.692, Val. Loss: 0.782, Val Acc.: 71.000\n",
      "Train loss: 0.678, Val. Loss: 0.791, Val Acc.: 72.000\n",
      "Train loss: 0.683, Val. Loss: 0.788, Val Acc.: 71.000\n",
      "Train loss: 0.689, Val. Loss: 0.777, Val Acc.: 71.000\n",
      "Train loss: 0.688, Val. Loss: 0.780, Val Acc.: 71.000\n",
      "Train loss: 0.691, Val. Loss: 0.777, Val Acc.: 71.000\n",
      "Train loss: 0.689, Val. Loss: 0.782, Val Acc.: 71.000\n",
      "Train loss: 0.681, Val. Loss: 0.770, Val Acc.: 72.000\n",
      "Train loss: 0.685, Val. Loss: 0.784, Val Acc.: 72.000\n",
      "Train loss: 0.685, Val. Loss: 0.783, Val Acc.: 71.000\n",
      "Train loss: 0.689, Val. Loss: 0.777, Val Acc.: 71.000\n",
      "Train loss: 0.680, Val. Loss: 0.770, Val Acc.: 72.000\n",
      "Train loss: 0.677, Val. Loss: 0.775, Val Acc.: 72.000\n",
      "Train loss: 0.678, Val. Loss: 0.770, Val Acc.: 72.000\n",
      "Train loss: 0.686, Val. Loss: 0.768, Val Acc.: 72.000\n",
      "Train loss: 0.679, Val. Loss: 0.776, Val Acc.: 71.000\n",
      "Train loss: 0.673, Val. Loss: 0.781, Val Acc.: 71.000\n",
      "Train loss: 0.685, Val. Loss: 0.771, Val Acc.: 72.000\n",
      "Starting epoch 2\n",
      "Train loss: 0.651, Val. Loss: 0.792, Val Acc.: 72.000\n",
      "Train loss: 0.663, Val. Loss: 0.784, Val Acc.: 72.000\n",
      "Train loss: 0.654, Val. Loss: 0.796, Val Acc.: 72.000\n",
      "Train loss: 0.654, Val. Loss: 0.789, Val Acc.: 72.000\n",
      "Train loss: 0.659, Val. Loss: 0.786, Val Acc.: 72.000\n",
      "Train loss: 0.660, Val. Loss: 0.779, Val Acc.: 72.000\n",
      "Train loss: 0.672, Val. Loss: 0.775, Val Acc.: 72.000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Embedding size is vocab size + 1 for sequence padding\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "epochs = 5\n",
    "    criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "print_every = 100\n",
    "batch_size = 256\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    for text_batch, labels in dataloader(train_features, train_labels, \n",
    "                                         batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        \n",
    "        # We're not passing the hidden state between batches, so create a new one here\n",
    "        hidden = model.init_hidden(labels.shape[0])\n",
    "        \n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logps, hidden = model(text_batch, hidden)\n",
    "        \n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_steps = 0\n",
    "            val_loss, accuracy = 0, 0\n",
    "            for text_batch, labels in dataloader(valid_features, valid_labels, batch_size=batch_size):\n",
    "                val_steps += 1\n",
    "                # We're not passing the hidden state between batches, so create a new one here\n",
    "                hidden = model.init_hidden(labels.shape[0])\n",
    "                \n",
    "                text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                logps, hidden = model(text_batch, hidden)\n",
    "                loss = criterion(logps, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                topval, topclass = torch.exp(logps).topk(1)\n",
    "                accuracy += torch.sum(topclass.squeeze() == labels)\n",
    "\n",
    "            print(f\"Train loss: {running_loss/print_every:.3f}, Val. Loss: {val_loss/val_steps:.3f}, Val Acc.: {100*accuracy/len(valid_labels):.3f}\")\n",
    "            model.train()\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Okay, now that you have a trained model, try it on some new tweets and see if it works appropriately. Remember that for any new text, you'll need to preprocess it first before passing it to the network. You should also think about how to handle input words that aren't in your vocabulary.\n",
    "\n",
    "We also want to use these sentiment scores in a larger ensemble model which you'll be learning about next. For this, we'll need the output to be some continuous value, typically on a scale of -3 to 3. Our model is predicting the probability on this discrete 5 value scale. Since we have a probability distribution, a good way to convert this to a continuous value is using the expectated value of the score. The expected value $\\bar{s}$ is the sum of each score $s_i$ multiplied by the probability $p_i$ of getting that score.\n",
    "\n",
    "$$\n",
    "\\large \\bar{s} = \\sum_i p_i s_i\n",
    "$$\n",
    "\n",
    "This is nice because it captures uncertainty in our model's predictions. For example, if it predicts 50% in positive ($s = 1$) and 50% in strongly positive ($s=2$), the expected value will be in between at 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    tokens = preprocess(text)\n",
    "    # Filter non-vocab words and convert to ids\n",
    "    tokens = [vocab[word] for word in tokens if word in vocab]\n",
    "    if len(tokens) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.tensor(tokens).view(-1, 1)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    logps, _ = model(text_input, hidden)\n",
    "    ps = torch.exp(logps)\n",
    "\n",
    "    # Sentiment expectation\n",
    "    expectation = torch.dot(torch.tensor([-2., -1., 0., 1., 2.]), ps.squeeze())\n",
    "    \n",
    "    return expectation.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Good earnings this year, I'm bullish on $goog\"\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, last part. Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
    "\n",
    "**Note from Mat:** I'm leaving this to Brok and Parnian to finish up. At this point we need to determine the universe of stocks/tickers students will use. I'm using data from the original StockTwits dataset, but we probably want something else. In this last part, we want to pretend students are getting a stream of twits and classifying them. Justin said the final output of the model should be a tickers, timestamps, and sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_twits.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide this stream for students\n",
    "def twit_stream():\n",
    "    for twit in test_data['data'][:1000]:\n",
    "        yield twit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" Given a stream of twits and a universe of tickers, return sentiment scores for \n",
    "        tickers in the universe. \"\"\"\n",
    "    \n",
    "    for twit in stream:\n",
    "        \n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "\n",
    "        score = predict(text, model, vocab)\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield symbols, score, twit['timestamp']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(score_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ideas \n",
    " 1. Add Digram \n",
    " 2. Add a Stock Twits - maybe record the video to watch how to train stock \n",
    " 3. Add a story board of what is happening \n",
    " 4. Breaking a code into piceses "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
